{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_L5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnx98VS6o__s"
      },
      "source": [
        "**Урок 5**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-EAUBjW6nTj",
        "outputId": "d283f16f-f53f-4150-8bc0-5574d092339d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB-SYbCkpKPc",
        "outputId": "f5b485b5-49b1-438a-b6a4-ee11c46284b0"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 67.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805911 sha256=c7fa37139f6daa0922e57e5346587741658bde5f2b51ab55ff73795d97d9a77d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcK8cwMIpszD"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.master('local[4]').appName('joins_hw').getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R3Bm-ycpyrJ"
      },
      "source": [
        "**Joins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuOyeDjzp3ue",
        "outputId": "e5d3c53f-a71a-401f-b606-c47191fecbc8"
      },
      "source": [
        "# Создаём датасет для примеров\n",
        "dataset_1 = [\n",
        "  {\n",
        "    'key_1' : 'abc',\n",
        "    'value_1' : 10,\n",
        "    'value_2' : 20\n",
        "  },\n",
        "  {\n",
        "    'key_1' : 'def',\n",
        "    'value_1' : 100,\n",
        "    'value_2' : 300\n",
        "  }\n",
        "]\n",
        "\n",
        "dataset_2 = [\n",
        "  {\n",
        "    'key_2' : 'abc',\n",
        "    'value_1' : 5.5,\n",
        "    'value_2' : 2.2\n",
        "  },\n",
        "  {\n",
        "    'key_2' : 'xyz',\n",
        "    'value_1' : 10.1,\n",
        "    'value_2' : 13.5\n",
        "  }\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(dataset_1)\n",
        "print('df1')\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame(dataset_2)\n",
        "print('df2')\n",
        "df2.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df1\n",
            "+-----+-------+-------+\n",
            "|key_1|value_1|value_2|\n",
            "+-----+-------+-------+\n",
            "|  abc|     10|     20|\n",
            "|  def|    100|    300|\n",
            "+-----+-------+-------+\n",
            "\n",
            "df2\n",
            "+-----+-------+-------+\n",
            "|key_2|value_1|value_2|\n",
            "+-----+-------+-------+\n",
            "|  abc|    5.5|    2.2|\n",
            "|  xyz|   10.1|   13.5|\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA7Auby4p_-T"
      },
      "source": [
        "**Left Semi join**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9NXGMIlqBwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf337e4-8904-420d-90d4-ceea3afc66f7"
      },
      "source": [
        "# Создайте джойн, чтобы получить следующую таблицу\n",
        "# +---+-------+-------+\n",
        "# |key|value_1|value_2|\n",
        "# +---+-------+-------+\n",
        "# |abc|     10|     20|\n",
        "# +---+-------+-------+\n",
        "\n",
        "df1.join(df2,df1.key_1 == df2.key_2,\"leftsemi\").show(truncate=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+\n",
            "|key_1|value_1|value_2|\n",
            "+-----+-------+-------+\n",
            "|abc  |10     |20     |\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuXIRDGVqWmV"
      },
      "source": [
        "**Left Anti join**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyzx6EECqYbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2e3d79-44ca-4c22-a385-e9d33ca9113e"
      },
      "source": [
        "# Создайте джойн, чтобы получить следующую таблицу\n",
        "# +---+-------+-------+\n",
        "# |key|value_1|value_2|\n",
        "# +---+-------+-------+\n",
        "# |def|    100|    300|\n",
        "# +---+-------+-------+\n",
        "\n",
        "df1.join(df2,df1.key_1 == df2.key_2,\"leftanti\").show(truncate=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+\n",
            "|key_1|value_1|value_2|\n",
            "+-----+-------+-------+\n",
            "|def  |100    |300    |\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgx2631vql9i"
      },
      "source": [
        "**Inner join**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdKZ278-qntb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308c7449-7f3a-4b1e-b671-6d9229773407"
      },
      "source": [
        "# Создайте Inner джойн с условиями ---hidden---, для df1 и df2, соответсвенно\n",
        "# В  итоге получится таблица\n",
        "# +---+-------+-------+---+-------+-------+\n",
        "# |key|value_1|value_2|key|value_1|value_2|\n",
        "# +---+-------+-------+---+-------+-------+\n",
        "# |abc|     10|     20|abc|    5.5|    2.2|\n",
        "# |abc|     10|     20|xyz|   10.1|   13.5|\n",
        "# |def|    100|    300|abc|    5.5|    2.2|\n",
        "# |def|    100|    300|xyz|   10.1|   13.5|\n",
        "# +---+-------+-------+---+-------+-------+\n",
        "\n",
        "#df1.join(df2, [df1.key_1 == df2.key_2|df1.key_1 ~ df2.key_2],\"inner\").show(truncate=False)\n",
        "\n",
        "df1.join(df2, [(df1.key_1 == df2.key_2) | (df1.key_1 != df2.key_2)],\"inner\").show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+-----+-------+-------+\n",
            "|key_1|value_1|value_2|key_2|value_1|value_2|\n",
            "+-----+-------+-------+-----+-------+-------+\n",
            "|  abc|     10|     20|  abc|    5.5|    2.2|\n",
            "|  abc|     10|     20|  xyz|   10.1|   13.5|\n",
            "|  def|    100|    300|  abc|    5.5|    2.2|\n",
            "|  def|    100|    300|  xyz|   10.1|   13.5|\n",
            "+-----+-------+-------+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}